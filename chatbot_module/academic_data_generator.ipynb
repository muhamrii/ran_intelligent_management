{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d52a42bf",
   "metadata": {},
   "source": [
    "# Academic Benchmarking Data Generator\n",
    "\n",
    "This notebook generates high-quality sample ground truth data for academic IR and NLU benchmarking based on the actual knowledge graph structure and domain knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ee209c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import List, Dict, Tuple\n",
    "import re\n",
    "\n",
    "# Add parent directory to path\n",
    "parent_dir = os.path.dirname(os.path.dirname(os.path.abspath('.')))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "\n",
    "from knowledge_graph_module.kg_builder import RANNeo4jIntegrator\n",
    "from chatbot_module.chatbot import EnhancedRANChatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7cad48",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41224ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neo4j connection settings\n",
    "NEO4J_URI = \"bolt://localhost:7687\"\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"neo4j\"\n",
    "\n",
    "# Initialize connections\n",
    "try:\n",
    "    integrator = RANNeo4jIntegrator(NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD)\n",
    "    chatbot = EnhancedRANChatbot(integrator, use_domain_model=True)\n",
    "    print(\"âœ… Connected to Neo4j and initialized chatbot\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Connection failed: {e}\")\n",
    "    print(\"Please ensure Neo4j is running and accessible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1874cd",
   "metadata": {},
   "source": [
    "## Explore Knowledge Graph Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e317ab4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get schema overview\n",
    "schema = integrator.get_schema_overview()\n",
    "print(f\"Total tables: {schema.get('tables', {}).get('total_tables', 0)}\")\n",
    "print(f\"Total columns: {schema.get('columns', {}).get('total_columns', 0)}\")\n",
    "print(f\"Total concepts: {len(schema.get('concepts', []))}\")\n",
    "\n",
    "# Get sample tables for analysis\n",
    "sample_tables = schema.get('tables', {}).get('sample_tables', [])[:20]\n",
    "print(f\"\\nSample tables ({len(sample_tables)}):\")\n",
    "for table in sample_tables:\n",
    "    print(f\"- {table['table_name']} ({table['row_count']} rows)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abef25f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get domain concepts for better query generation\n",
    "concepts = schema.get('concepts', [])\n",
    "print(f\"Available concepts ({len(concepts)}):\")\n",
    "for concept in concepts[:15]:\n",
    "    print(f\"- {concept}\")\n",
    "\n",
    "# Get relationship patterns\n",
    "relationships = schema.get('relationships', [])\n",
    "print(f\"\\nRelationship patterns ({len(relationships)}):\")\n",
    "for rel in relationships[:10]:\n",
    "    print(f\"- {rel['type']}: {rel['count']} instances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087ad680",
   "metadata": {},
   "source": [
    "## Define Query Templates and Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99961066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAN domain-specific query templates\n",
    "QUERY_TEMPLATES = {\n",
    "    'power_analysis': [\n",
    "        \"Show me power consumption data from {table}\",\n",
    "        \"Find energy efficiency metrics in {table}.{column}\",\n",
    "        \"Get power optimization insights for {concept}\",\n",
    "        \"Display power measurements from {table}\",\n",
    "        \"Analyze energy consumption patterns in {concept}\"\n",
    "    ],\n",
    "    'frequency_management': [\n",
    "        \"Show frequency allocation data from {table}\",\n",
    "        \"Find spectrum usage in {table}.{column}\",\n",
    "        \"Get carrier frequency information from {concept}\",\n",
    "        \"Display frequency band data in {table}\",\n",
    "        \"Analyze spectrum efficiency for {concept}\"\n",
    "    ],\n",
    "    'performance_metrics': [\n",
    "        \"Show throughput data from {table}\",\n",
    "        \"Find KPI measurements in {table}.{column}\",\n",
    "        \"Get performance analysis for {concept}\",\n",
    "        \"Display quality metrics from {table}\",\n",
    "        \"Analyze network performance in {concept}\"\n",
    "    ],\n",
    "    'cell_configuration': [\n",
    "        \"Show cell parameters from {table}\",\n",
    "        \"Find configuration settings in {table}.{column}\",\n",
    "        \"Get cell setup information for {concept}\",\n",
    "        \"Display antenna configuration from {table}\",\n",
    "        \"Analyze cell optimization in {concept}\"\n",
    "    ],\n",
    "    'neighbor_relations': [\n",
    "        \"Show neighbor cell data from {table}\",\n",
    "        \"Find handover information in {table}.{column}\",\n",
    "        \"Get neighbor relations for {concept}\",\n",
    "        \"Display adjacency data from {table}\",\n",
    "        \"Analyze cell relationships in {concept}\"\n",
    "    ],\n",
    "    'timing_sync': [\n",
    "        \"Show timing synchronization from {table}\",\n",
    "        \"Find sync parameters in {table}.{column}\",\n",
    "        \"Get timing data for {concept}\",\n",
    "        \"Display synchronization status from {table}\",\n",
    "        \"Analyze timing accuracy in {concept}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Common RAN table patterns\n",
    "RAN_TABLE_PATTERNS = {\n",
    "    'power': ['power', 'energy', 'consumption', 'dbm', 'watt'],\n",
    "    'frequency': ['frequency', 'freq', 'spectrum', 'band', 'carrier', 'eutra'],\n",
    "    'performance': ['throughput', 'kpi', 'quality', 'performance', 'metric'],\n",
    "    'cell': ['cell', 'sector', 'site', 'antenna', 'config'],\n",
    "    'neighbor': ['neighbor', 'relation', 'handover', 'adjacency'],\n",
    "    'sync': ['sync', 'timing', 'synchronization', 'time'],\n",
    "    'measurement': ['measurement', 'monitor', 'report', 'sample'],\n",
    "    'optimization': ['optimization', 'tuning', 'adjustment', 'parameter']\n",
    "}\n",
    "\n",
    "print(\"Query templates and patterns defined\")\n",
    "print(f\"Templates: {list(QUERY_TEMPLATES.keys())}\")\n",
    "print(f\"Patterns: {list(RAN_TABLE_PATTERNS.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a590fe60",
   "metadata": {},
   "source": [
    "## Generate IR Ground Truth Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b553be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_table(table_name: str, table_info: dict = None) -> str:\n",
    "    \"\"\"Categorize table based on name and content\"\"\"\n",
    "    name_lower = table_name.lower()\n",
    "    \n",
    "    for category, keywords in RAN_TABLE_PATTERNS.items():\n",
    "        if any(keyword in name_lower for keyword in keywords):\n",
    "            return category\n",
    "    \n",
    "    return 'general'\n",
    "\n",
    "def find_related_tables(target_table: str, all_tables: list, max_related: int = 5) -> list:\n",
    "    \"\"\"Find tables related to the target table\"\"\"\n",
    "    category = categorize_table(target_table)\n",
    "    related = []\n",
    "    \n",
    "    for table in all_tables:\n",
    "        if table['table_name'] != target_table:\n",
    "            if categorize_table(table['table_name']) == category:\n",
    "                related.append(table['table_name'])\n",
    "    \n",
    "    # Add some cross-category relationships for realism\n",
    "    cross_category_maps = {\n",
    "        'power': ['performance', 'cell'],\n",
    "        'frequency': ['performance', 'cell'],\n",
    "        'cell': ['neighbor', 'performance'],\n",
    "        'performance': ['power', 'frequency']\n",
    "    }\n",
    "    \n",
    "    if category in cross_category_maps:\n",
    "        for cross_cat in cross_category_maps[category]:\n",
    "            for table in all_tables:\n",
    "                if categorize_table(table['table_name']) == cross_cat:\n",
    "                    related.append(table['table_name'])\n",
    "    \n",
    "    return list(set(related))[:max_related]\n",
    "\n",
    "def generate_ir_ground_truth(num_queries: int = 50) -> pd.DataFrame:\n",
    "    \"\"\"Generate IR ground truth data\"\"\"\n",
    "    ir_data = []\n",
    "    tables = schema.get('tables', {}).get('sample_tables', [])\n",
    "    concepts = schema.get('concepts', [])\n",
    "    \n",
    "    random.seed(42)  # For reproducibility\n",
    "    \n",
    "    for i in range(num_queries):\n",
    "        # Select random category and template\n",
    "        category = random.choice(list(QUERY_TEMPLATES.keys()))\n",
    "        template = random.choice(QUERY_TEMPLATES[category])\n",
    "        \n",
    "        # Select target table based on category\n",
    "        category_pattern = category.split('_')[0] if '_' in category else category\n",
    "        relevant_tables = []\n",
    "        \n",
    "        # Find tables matching the category\n",
    "        for table in tables:\n",
    "            table_category = categorize_table(table['table_name'])\n",
    "            if (table_category == category_pattern or \n",
    "                category_pattern in table['table_name'].lower() or\n",
    "                any(keyword in table['table_name'].lower() \n",
    "                    for keyword in RAN_TABLE_PATTERNS.get(category_pattern, []))):\n",
    "                relevant_tables.append(table['table_name'])\n",
    "        \n",
    "        if not relevant_tables:\n",
    "            # Fallback to random table\n",
    "            target_table = random.choice(tables)['table_name']\n",
    "            relevant_tables = [target_table]\n",
    "        else:\n",
    "            target_table = random.choice(relevant_tables)\n",
    "        \n",
    "        # Generate query\n",
    "        if '{table}' in template:\n",
    "            query = template.format(table=target_table)\n",
    "        elif '{concept}' in template:\n",
    "            concept = random.choice(concepts) if concepts else target_table\n",
    "            query = template.format(concept=concept)\n",
    "            # Find tables related to this concept\n",
    "            concept_tables = []\n",
    "            for table in tables:\n",
    "                if (concept.lower() in table['table_name'].lower() or\n",
    "                    any(word in table['table_name'].lower() for word in concept.lower().split())):\n",
    "                    concept_tables.append(table['table_name'])\n",
    "            relevant_tables = concept_tables if concept_tables else relevant_tables\n",
    "        elif '{column}' in template:\n",
    "            # Get a sample column from target table\n",
    "            try:\n",
    "                table_details = integrator.get_table_details(target_table)\n",
    "                columns = table_details.get('columns', [])\n",
    "                if columns:\n",
    "                    sample_column = random.choice(columns)['name']\n",
    "                    query = template.format(table=target_table, column=sample_column)\n",
    "                else:\n",
    "                    query = template.replace('.{column}', '').format(table=target_table)\n",
    "            except:\n",
    "                query = template.replace('.{column}', '').format(table=target_table)\n",
    "        else:\n",
    "            query = template\n",
    "        \n",
    "        # Find additional related tables\n",
    "        related_tables = find_related_tables(target_table, tables)\n",
    "        all_relevant = list(set([target_table] + relevant_tables[:2] + related_tables[:3]))\n",
    "        \n",
    "        ir_data.append({\n",
    "            'query': query,\n",
    "            'relevant_tables': ','.join(all_relevant[:5]),  # Limit to top 5\n",
    "            'primary_table': target_table,\n",
    "            'category': category,\n",
    "            'num_relevant': len(all_relevant)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(ir_data)\n",
    "\n",
    "# Generate IR data\n",
    "ir_df = generate_ir_ground_truth(50)\n",
    "print(f\"Generated {len(ir_df)} IR queries\")\n",
    "print(f\"Categories: {ir_df['category'].value_counts().to_dict()}\")\n",
    "ir_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cb9338",
   "metadata": {},
   "source": [
    "## Generate NLU Ground Truth Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecfa283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_expected_answer(query: str, primary_table: str, category: str) -> str:\n",
    "    \"\"\"Generate expected answer based on query and context\"\"\"\n",
    "    \n",
    "    # Template responses based on category\n",
    "    answer_templates = {\n",
    "        'power_analysis': [\n",
    "            f\"ðŸ“Š Power analysis from {primary_table} shows energy consumption patterns across network elements. Key metrics include power efficiency ratios and consumption trends.\",\n",
    "            f\"âš¡ Energy data from {primary_table} reveals optimization opportunities in power management systems with detailed consumption measurements.\",\n",
    "            f\"ðŸ”‹ Power consumption analysis indicates efficiency levels from {primary_table} with recommendations for energy optimization.\"\n",
    "        ],\n",
    "        'frequency_management': [\n",
    "            f\"ðŸ“¡ Frequency data from {primary_table} displays spectrum allocation and carrier management information across bands.\",\n",
    "            f\"ðŸŒ Spectrum analysis from {primary_table} shows frequency utilization patterns and band efficiency metrics.\",\n",
    "            f\"ðŸ“Š Carrier frequency information from {primary_table} includes allocation status and spectrum optimization insights.\"\n",
    "        ],\n",
    "        'performance_metrics': [\n",
    "            f\"ðŸ“ˆ Performance metrics from {primary_table} show throughput, latency, and quality indicators across network elements.\",\n",
    "            f\"ðŸŽ¯ KPI analysis from {primary_table} reveals network performance trends and optimization opportunities.\",\n",
    "            f\"ðŸ“Š Quality measurements from {primary_table} include RSRP, RSRQ, and throughput performance data.\"\n",
    "        ],\n",
    "        'cell_configuration': [\n",
    "            f\"ðŸ—ï¸ Cell configuration from {primary_table} shows parameter settings, antenna configurations, and optimization values.\",\n",
    "            f\"âš™ï¸ Configuration data from {primary_table} includes cell parameters, thresholds, and antenna settings.\",\n",
    "            f\"ðŸ“‹ Cell setup information from {primary_table} displays optimization parameters and configuration status.\"\n",
    "        ],\n",
    "        'neighbor_relations': [\n",
    "            f\"ðŸ”— Neighbor relations from {primary_table} show cell adjacency patterns and handover optimization data.\",\n",
    "            f\"ðŸ¤ Handover analysis from {primary_table} reveals neighbor cell relationships and mobility patterns.\",\n",
    "            f\"ðŸ“Š Cell relationship data from {primary_table} includes neighbor configurations and handover statistics.\"\n",
    "        ],\n",
    "        'timing_sync': [\n",
    "            f\"â° Timing synchronization from {primary_table} shows sync accuracy and timing parameter configurations.\",\n",
    "            f\"ðŸ• Sync data from {primary_table} reveals timing accuracy measurements and synchronization status.\",\n",
    "            f\"ðŸ“Š Timing analysis from {primary_table} includes sync parameters and accuracy metrics.\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    if category in answer_templates:\n",
    "        return random.choice(answer_templates[category])\n",
    "    else:\n",
    "        return f\"ðŸ“‹ Data from {primary_table} provides insights into {category.replace('_', ' ')} with relevant metrics and analysis.\"\n",
    "\n",
    "def extract_entities_from_query(query: str, primary_table: str) -> list:\n",
    "    \"\"\"Extract key entities that should appear in responses\"\"\"\n",
    "    entities = [primary_table]\n",
    "    \n",
    "    # Common RAN entities\n",
    "    ran_entities = {\n",
    "        'power': ['consumedEnergyMeasurement', 'powerOptimization', 'energyEfficiency'],\n",
    "        'frequency': ['EUtranFrequency', 'freqBand', 'carrierFreq', 'spectrumAllocation'],\n",
    "        'performance': ['throughputMeasurement', 'qualityIndicator', 'performanceCounter'],\n",
    "        'cell': ['cellConfiguration', 'antennaConfig', 'cellParameters'],\n",
    "        'neighbor': ['neighborRelation', 'handoverConfig', 'adjacencyList'],\n",
    "        'sync': ['synchronizationConfig', 'timingAccuracy', 'syncStatus']\n",
    "    }\n",
    "    \n",
    "    query_lower = query.lower()\n",
    "    for category, entity_list in ran_entities.items():\n",
    "        if category in query_lower:\n",
    "            entities.extend(random.sample(entity_list, min(2, len(entity_list))))\n",
    "    \n",
    "    # Extract table.column patterns from query\n",
    "    table_column_pattern = r'([A-Za-z][A-Za-z0-9_]*)\\.[A-Za-z][A-Za-z0-9_]*'\n",
    "    matches = re.findall(table_column_pattern, query)\n",
    "    entities.extend(matches)\n",
    "    \n",
    "    return list(set(entities))[:5]  # Limit to 5 entities\n",
    "\n",
    "def generate_nlu_ground_truth(ir_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Generate NLU ground truth based on IR queries\"\"\"\n",
    "    nlu_data = []\n",
    "    \n",
    "    for _, row in ir_df.iterrows():\n",
    "        query = row['query']\n",
    "        primary_table = row['primary_table']\n",
    "        category = row['category']\n",
    "        \n",
    "        # Generate expected answer\n",
    "        answer = generate_expected_answer(query, primary_table, category)\n",
    "        \n",
    "        # Extract expected entities\n",
    "        entities = extract_entities_from_query(query, primary_table)\n",
    "        \n",
    "        nlu_data.append({\n",
    "            'query': query,\n",
    "            'answer': answer,\n",
    "            'entities': ','.join(entities),\n",
    "            'intent': category,\n",
    "            'complexity': 'medium' if len(entities) > 3 else 'simple',\n",
    "            'domain': 'RAN'\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(nlu_data)\n",
    "\n",
    "# Generate NLU data\n",
    "nlu_df = generate_nlu_ground_truth(ir_df)\n",
    "print(f\"Generated {len(nlu_df)} NLU examples\")\n",
    "print(f\"Intent distribution: {nlu_df['intent'].value_counts().to_dict()}\")\n",
    "print(f\"Complexity distribution: {nlu_df['complexity'].value_counts().to_dict()}\")\n",
    "nlu_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c767e9",
   "metadata": {},
   "source": [
    "## Validate Generated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec65202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate IR data\n",
    "print(\"=== IR Data Validation ===\")\n",
    "print(f\"Total queries: {len(ir_df)}\")\n",
    "print(f\"Unique queries: {ir_df['query'].nunique()}\")\n",
    "print(f\"Average relevant tables per query: {ir_df['num_relevant'].mean():.1f}\")\n",
    "print(f\"Categories covered: {sorted(ir_df['category'].unique())}\")\n",
    "\n",
    "# Check for missing data\n",
    "print(f\"\\nMissing data:\")\n",
    "print(f\"- Queries with no relevant tables: {(ir_df['relevant_tables'] == '').sum()}\")\n",
    "print(f\"- Queries with no primary table: {ir_df['primary_table'].isnull().sum()}\")\n",
    "\n",
    "# Sample validation\n",
    "print(f\"\\nSample IR entries:\")\n",
    "for i, row in ir_df.sample(3).iterrows():\n",
    "    print(f\"Q: {row['query']}\")\n",
    "    print(f\"   Relevant: {row['relevant_tables']}\")\n",
    "    print(f\"   Primary: {row['primary_table']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28a06c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate NLU data\n",
    "print(\"=== NLU Data Validation ===\")\n",
    "print(f\"Total examples: {len(nlu_df)}\")\n",
    "print(f\"Unique queries: {nlu_df['query'].nunique()}\")\n",
    "print(f\"Average entities per query: {nlu_df['entities'].apply(lambda x: len(x.split(',')) if x else 0).mean():.1f}\")\n",
    "print(f\"Average answer length: {nlu_df['answer'].apply(len).mean():.0f} chars\")\n",
    "\n",
    "# Check for missing data\n",
    "print(f\"\\nMissing data:\")\n",
    "print(f\"- Queries with no answer: {nlu_df['answer'].isnull().sum()}\")\n",
    "print(f\"- Queries with no entities: {(nlu_df['entities'] == '').sum()}\")\n",
    "\n",
    "# Sample validation\n",
    "print(f\"\\nSample NLU entries:\")\n",
    "for i, row in nlu_df.sample(2).iterrows():\n",
    "    print(f\"Q: {row['query']}\")\n",
    "    print(f\"A: {row['answer'][:100]}...\")\n",
    "    print(f\"Entities: {row['entities']}\")\n",
    "    print(f\"Intent: {row['intent']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac816f9",
   "metadata": {},
   "source": [
    "## Test with Real Chatbot Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0679268a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a few queries with the actual chatbot to ensure realism\n",
    "print(\"=== Testing with Real Chatbot ===\")\n",
    "\n",
    "test_queries = ir_df.sample(3)['query'].tolist()\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    try:\n",
    "        result = chatbot.enhanced_process_query(query)\n",
    "        response = result.get('response') or chatbot.generate_response(result)\n",
    "        print(f\"Response: {response[:200]}...\")\n",
    "        print(f\"Type: {result.get('type')}\")\n",
    "        \n",
    "        # Extract retrieved tables\n",
    "        retrieved_tables = []\n",
    "        if result.get('type') == 'semantic_search':\n",
    "            for r in (result.get('results') or [])[:5]:\n",
    "                if r.get('table_name'):\n",
    "                    retrieved_tables.append(r['table_name'])\n",
    "        print(f\"Retrieved tables: {retrieved_tables}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8ee10b",
   "metadata": {},
   "source": [
    "## Save Generated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74f1446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV files\n",
    "output_dir = os.path.dirname(os.path.dirname(os.path.abspath('.')))\n",
    "\n",
    "# Save IR ground truth\n",
    "ir_output_path = os.path.join(output_dir, 'sample_ir_ground_truth.csv')\n",
    "ir_export_df = ir_df[['query', 'relevant_tables']].copy()\n",
    "ir_export_df.to_csv(ir_output_path, index=False)\n",
    "print(f\"âœ… IR ground truth saved to: {ir_output_path}\")\n",
    "print(f\"   Shape: {ir_export_df.shape}\")\n",
    "\n",
    "# Save NLU ground truth\n",
    "nlu_output_path = os.path.join(output_dir, 'sample_nlu_ground_truth.csv')\n",
    "nlu_export_df = nlu_df[['query', 'answer', 'entities']].copy()\n",
    "nlu_export_df.to_csv(nlu_output_path, index=False)\n",
    "print(f\"âœ… NLU ground truth saved to: {nlu_output_path}\")\n",
    "print(f\"   Shape: {nlu_export_df.shape}\")\n",
    "\n",
    "# Save detailed analysis data (optional)\n",
    "ir_detailed_path = os.path.join(output_dir, 'detailed_ir_analysis.csv')\n",
    "ir_df.to_csv(ir_detailed_path, index=False)\n",
    "print(f\"âœ… Detailed IR analysis saved to: {ir_detailed_path}\")\n",
    "\n",
    "nlu_detailed_path = os.path.join(output_dir, 'detailed_nlu_analysis.csv')\n",
    "nlu_df.to_csv(nlu_detailed_path, index=False)\n",
    "print(f\"âœ… Detailed NLU analysis saved to: {nlu_detailed_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00211a55",
   "metadata": {},
   "source": [
    "## Generate Additional High-Quality Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc9dc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate additional samples with more sophisticated patterns\n",
    "def generate_advanced_ir_samples(num_samples: int = 25) -> pd.DataFrame:\n",
    "    \"\"\"Generate more sophisticated IR samples using actual table relationships\"\"\"\n",
    "    advanced_samples = []\n",
    "    \n",
    "    # Get actual relationships from the KG\n",
    "    try:\n",
    "        relationships = integrator.get_table_relationships(limit=20)\n",
    "    except:\n",
    "        relationships = []\n",
    "    \n",
    "    # Advanced query patterns\n",
    "    advanced_patterns = [\n",
    "        \"Correlate {table1} with {table2} for optimization insights\",\n",
    "        \"Cross-reference {table1} and {table2} performance data\",\n",
    "        \"Compare metrics between {table1} and {table2}\",\n",
    "        \"Find relationships linking {table1} to {table2}\",\n",
    "        \"Analyze joint patterns in {table1} and {table2}\",\n",
    "        \"Show dependencies between {table1} and {table2}\"\n",
    "    ]\n",
    "    \n",
    "    tables = [t['table_name'] for t in schema.get('tables', {}).get('sample_tables', [])]\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        if relationships and random.random() > 0.5:\n",
    "            # Use actual relationship\n",
    "            rel = random.choice(relationships)\n",
    "            table1 = rel.get('table1', random.choice(tables))\n",
    "            table2 = rel.get('table2', random.choice(tables))\n",
    "        else:\n",
    "            # Random pair\n",
    "            table1, table2 = random.sample(tables, 2)\n",
    "        \n",
    "        pattern = random.choice(advanced_patterns)\n",
    "        query = pattern.format(table1=table1, table2=table2)\n",
    "        \n",
    "        # Find related tables based on categories\n",
    "        cat1 = categorize_table(table1)\n",
    "        cat2 = categorize_table(table2)\n",
    "        \n",
    "        relevant_tables = [table1, table2]\n",
    "        \n",
    "        # Add tables from same categories\n",
    "        for table in tables:\n",
    "            if (categorize_table(table) in [cat1, cat2] and \n",
    "                table not in relevant_tables and \n",
    "                len(relevant_tables) < 6):\n",
    "                relevant_tables.append(table)\n",
    "        \n",
    "        advanced_samples.append({\n",
    "            'query': query,\n",
    "            'relevant_tables': ','.join(relevant_tables),\n",
    "            'primary_table': table1,\n",
    "            'category': 'multi_table_analysis',\n",
    "            'num_relevant': len(relevant_tables)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(advanced_samples)\n",
    "\n",
    "# Generate advanced samples\n",
    "advanced_ir_df = generate_advanced_ir_samples(25)\n",
    "print(f\"Generated {len(advanced_ir_df)} advanced IR samples\")\n",
    "\n",
    "# Combine with original samples\n",
    "combined_ir_df = pd.concat([ir_df, advanced_ir_df], ignore_index=True)\n",
    "print(f\"Total IR samples: {len(combined_ir_df)}\")\n",
    "\n",
    "# Generate corresponding NLU samples\n",
    "advanced_nlu_df = generate_nlu_ground_truth(advanced_ir_df)\n",
    "combined_nlu_df = pd.concat([nlu_df, advanced_nlu_df], ignore_index=True)\n",
    "print(f\"Total NLU samples: {len(combined_nlu_df)}\")\n",
    "\n",
    "# Save updated files\n",
    "combined_ir_df[['query', 'relevant_tables']].to_csv(ir_output_path, index=False)\n",
    "combined_nlu_df[['query', 'answer', 'entities']].to_csv(nlu_output_path, index=False)\n",
    "\n",
    "print(f\"\\nâœ… Updated files saved with enhanced samples\")\n",
    "print(f\"IR samples: {len(combined_ir_df)}\")\n",
    "print(f\"NLU samples: {len(combined_nlu_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023975a9",
   "metadata": {},
   "source": [
    "## Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68e05e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Generation Summary ===\")\n",
    "print(f\"ðŸ“ Generated files:\")\n",
    "print(f\"   â€¢ sample_ir_ground_truth.csv - {len(combined_ir_df)} IR queries\")\n",
    "print(f\"   â€¢ sample_nlu_ground_truth.csv - {len(combined_nlu_df)} NLU examples\")\n",
    "print(f\"   â€¢ detailed_ir_analysis.csv - Full IR analysis\")\n",
    "print(f\"   â€¢ detailed_nlu_analysis.csv - Full NLU analysis\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Coverage:\")\n",
    "print(f\"   â€¢ Categories: {len(combined_ir_df['category'].unique())} unique\")\n",
    "print(f\"   â€¢ Tables: {len(set(','.join(combined_ir_df['relevant_tables']).split(',')))} referenced\")\n",
    "print(f\"   â€¢ Avg entities/query: {combined_nlu_df['entities'].apply(lambda x: len(x.split(',')) if x else 0).mean():.1f}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Quality metrics:\")\n",
    "print(f\"   â€¢ Unique queries: {combined_ir_df['query'].nunique()}/{len(combined_ir_df)} ({100*combined_ir_df['query'].nunique()/len(combined_ir_df):.1f}%)\")\n",
    "print(f\"   â€¢ Avg relevant tables: {combined_ir_df['num_relevant'].mean():.1f}\")\n",
    "print(f\"   â€¢ Complex queries: {(combined_nlu_df['complexity'] == 'medium').sum()}/{len(combined_nlu_df)}\")\n",
    "\n",
    "print(f\"\\nðŸš€ Next steps:\")\n",
    "print(f\"   1. Run the UI with 'Use generated sample data' option\")\n",
    "print(f\"   2. Test academic benchmarking with the generated data\")\n",
    "print(f\"   3. Fine-tune query patterns based on results\")\n",
    "print(f\"   4. Generate domain-specific variants as needed\")\n",
    "\n",
    "print(f\"\\nâœ… Data generation complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
